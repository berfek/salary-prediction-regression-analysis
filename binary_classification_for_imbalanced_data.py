# -*- coding: utf-8 -*-
"""Binary Classification for Imbalanced Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1id2T2vExR-1S81bKNv7yImfMjA2Xibiq

# **Binary Classification for Imbalanced Data**

I have data on 31K historical train records spanning over a period of 3 days. I need to create a model for classification. "Delayed" feature is a binary target column, 0 indicates "No Delay" and "1" indicates "Delay".  

First, since the dataset is in nested format, I normalized "Stop" feature and included "Headcode" from the columns.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV
from sklearn.metrics import classification_report
import json

"""Read historical dataset:"""

with open('./raildataset.json','r') as f:
    data = json.loads(f.read())

normalized_df = pd.json_normalize(
    data,
    record_path=['Stops'],
    meta=['Headcode'], errors='ignore'
)

print(normalized_df.info())

"""Read coordinates dataset and merge it with historical:"""

with open('./station_coordinates5.json','r') as f:
    coordinates = json.loads(f.read())

dfcoord = pd.json_normalize(
    coordinates,
    record_path=['stations'],
    errors='ignore'
)
dfcoord.head()

dfcoord.rename(columns={"crs": "CRS"}, inplace=True)
normalized_df.rename(columns={"Crs": "CRS"}, inplace=True)

normalized_df = pd.merge(normalized_df, dfcoord, on='CRS', how='left')

"""Eliminated null values from the dataset:

"""

normalized_df = normalized_df.dropna()

"""Eliminated null date values:"""

normalized_df = normalized_df[
    (normalized_df['ActualDeparture.$date'] != '0001-01-03T00:00:00.000Z') &
    (normalized_df['ActualArrival.$date'] != '0001-01-03T00:00:00.000Z')
]

"""Converted date columns to date format since they are in object format now:"""

normalized_df['BookedArrival.$date'] = pd.to_datetime(normalized_df['BookedArrival.$date'])

"""Created sin_time and cos_time features as the nature of date is cyclical (To explore more about cyclical features, check my work "Encoding cyclical features"):"""

normalized_df['hour'] = normalized_df['BookedArrival.$date'].dt.hour
normalized_df['minute'] = normalized_df['BookedArrival.$date'].dt.minute
normalized_df['second'] = normalized_df['BookedArrival.$date'].dt.second
normalized_df['hourfloat']=normalized_df.hour+normalized_df.minute/60.0+ normalized_df.second/3600.0

normalized_df['sin_time']=np.sin(2.*np.pi*normalized_df.hourfloat/24.)
normalized_df['cos_time']=np.cos(2.*np.pi*normalized_df.hourfloat/24.)

"""Created day_of_week and encoded it to use as an input value in classification:"""

normalized_df['day_of_week'] = normalized_df['BookedArrival.$date'].dt.day_name()

lblEncoder_day = LabelEncoder()
normalized_df['day_of_week'] = lblEncoder_day.fit_transform(normalized_df['day_of_week'])
print(lblEncoder_day.classes_)
print(lblEncoder_day.transform(lblEncoder_day.classes_))

"""Check class weights:"""

target_counts = normalized_df['Delayed'].value_counts()

print(target_counts)

"""I used "compute_class_weight" function from scikit-learn, which calculates class weights. A lower weight assigned to a class implies its greater prevalence, while a higher weight signifies a less frequent class occurrence."""

normalized_df['Delayed'] = normalized_df['Delayed'].astype(int)

class_weights = compute_class_weight('balanced', classes=normalized_df['Delayed'].unique(), y=normalized_df['Delayed'])

class_weights_dict = dict(zip(normalized_df['Delayed'].unique(), class_weights))

print("Class Weights:")
print(class_weights_dict)

"""As we see, the target value is quite imbalanced.  As evident from the output, class 1 is minority, while class 0 is majority. This implies that the model will be more penalized for misclassifying samples of class 1. The goal is to balance the impact of the minority class (class 1) against the majority class (class 0), allowing the model to learn to distinguish both classes effectively.

To address this imbalance, I performed a technique called Random Over-sampling to generate additional samples for the minority classes, thereby equalizing the instance counts across all classes
"""

pip install imbalanced-learn

normalized_df.head()

import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils.class_weight import compute_class_weight

cols = ['DwellBooked', 'DwellActual', 'StationNumber', 'sin_time', 'cos_time','day_of_week','X_y','Y_y']
X = normalized_df[cols]

y = normalized_df['Delayed']
y = y.to_frame()

#Convert 'DelayCategory' column to integers if it's not already
y['Delayed'] = y['Delayed'].astype(int)

#Apply oversampling
oversampler = RandomOverSampler(random_state=42)
X_resampled, y_resampled = oversampler.fit_resample(X, y)

#Convert the oversampled arrays back to DataFrame
X_resampled = pd.DataFrame(X_resampled, columns=cols)
y_resampled = pd.DataFrame(y_resampled, columns=['Delayed'])

#XCheck the class distribution after oversampling
print("Class Distribution after Oversampling:")
print(y_resampled['Delayed'].value_counts())

"""After training the model using the resampled data, it created a larger dataset consisting of 16,687 for both classes.

Split dataset into training and test with the ratio of 0.80 - 0.20:
"""

Xtrain, Xtest, ytrain, ytest = train_test_split(
    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42
)

"""Applied Random Forest since it is good when we have imbalanced dataset:"""

#Evaluating feature importance.
clf = RandomForestClassifier(n_estimators=200)
clf = clf.fit(Xtrain, ytrain)
indices = np.argsort(clf.feature_importances_)[::-1]

print('Feature ranking:')

for f in range(Xtrain.shape[1]):
    print('%d. feature %d %s (%f)' % (f + 1, indices[f], Xtrain.columns[indices[f]],
                                      clf.feature_importances_[indices[f]]))

"""Some features have less importance, however, I don't eliminate them due to the class imbalance, as less important factors could be important for predicting "1".


"""

clf = RandomForestClassifier(n_estimators=150, n_jobs=-1, criterion = 'gini', max_features = 'sqrt',
                             min_samples_split=7, min_weight_fraction_leaf=0.0,
                             max_leaf_nodes=40, max_depth=10)

calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=5)
calibrated_clf.fit(Xtrain, ytrain)
y_val = calibrated_clf.predict_proba(Xtest)

print(classification_report(ytest, pd.DataFrame(y_val).idxmax(axis=1).values, target_names=['0', '1'], digits=4))

"""Now we can see that the models give good predictions for both classes, and the overall accuracy is 71%."""